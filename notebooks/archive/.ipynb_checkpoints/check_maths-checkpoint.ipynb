{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# SV-Softmax math"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/dist-packages/requests/__init__.py:83: RequestsDependencyWarning: Old version of cryptography ([1, 2, 3]) may cause slowdown.\n",
      "  warnings.warn(warning, RequestsDependencyWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import array_ops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Support vector guided softmax loss - is a novel loss function which adaptively emphasizes the mis-classified points (support vectors) to guide the discriminative features learning. It makes it close to hard negative mining and the Focal loss techniques.\n",
    "\n",
    "Let's define a binary mask to adaptively indicate whether a sample is selected as the support vector by a specific classifier in the current stage. To the end, the binary mask is defined as follows:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "$$\n",
    "I_k = \\left\\{\n",
    "        \\begin{array}{ll}\n",
    "            0, & \\quad \\cos(\\theta_{w_y}, x) − \\cos(\\theta_{w_k}, x) \\ge 0 \\\\\n",
    "            1, & \\quad \\cos(\\theta_{w_y}, x) − \\cos(\\theta_{w_k}, x) < 0\n",
    "        \\end{array}\n",
    "      \\right.\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "where $\\cos(\\theta_{w_k}, x) = w_k^Tx$ is the cosine similarity and $θ_{w_k,x}$ is the angle between $w_k$ and $x$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Logits:\n",
      "[[ 2.   3.   1.  -1. ]\n",
      " [-1.   2.1  2.   6. ]\n",
      " [-2.   3.   4.  -2.1]]\n",
      "\n",
      "GT:\n",
      "[[0. 1. 0. 0.]\n",
      " [0. 0. 1. 0.]\n",
      " [1. 0. 0. 0.]]\n",
      "\n",
      "Binary mask:\n",
      "[[0. 0. 0. 0.]\n",
      " [0. 1. 0. 1.]\n",
      " [0. 1. 1. 0.]]\n"
     ]
    }
   ],
   "source": [
    "# placeholders\n",
    "logits = tf.placeholder(tf.float32)\n",
    "y_true = tf.placeholder(tf.float32)\n",
    "\n",
    "zeros = array_ops.zeros_like(logits, dtype=logits.dtype)\n",
    "ones = array_ops.ones_like(logits, dtype=logits.dtype)\n",
    "\n",
    "logit_y = tf.reduce_sum(tf.multiply(y_true, logits), axis=-1, keepdims=True)\n",
    "I_k = array_ops.where(logit_y >= logits, zeros, ones)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    logits_array = np.array([[2., 3., 1., -1.], [-1., 2.1, 2., 6], [-2., 3., 4, -2.1]])\n",
    "    y_true_array = np.array([[0., 1., 0., 0], [0., 0., 1., 0], [1., 0., 0., 0.]])\n",
    "    binary_mask = (sess.run(I_k, feed_dict={logits: logits_array, y_true: y_true_array}))\n",
    "    \n",
    "print(\"Logits:\")\n",
    "print(logits_array)\n",
    "print('')\n",
    "print(\"GT:\")\n",
    "print(y_true_array)\n",
    "print('')\n",
    "print(\"Binary mask:\")\n",
    "print(binary_mask)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's also define indicator function $h(t, θ_{w_k}, x, I_k)$ with preset hyperparameter t:\n",
    "\n",
    "$$h(t, θ_{w_k}, x, I_k) = e^{s(t−1)(\\cos(\\theta_{w_k, x})+1)I_k}$$\n",
    "\n",
    "Obviously, when t = 1, the designed SV-Softmax loss becomes identical to the original softmax loss. Let's implement it in a naive way."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "h with t=1.2:\n",
      "[[1.        1.        1.        1.       ]\n",
      " [1.        1.8589282 1.        4.055201 ]\n",
      " [1.        2.2255414 2.7182825 1.       ]]\n",
      "\n",
      "h with t=1.0:\n",
      "[[1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]\n",
      " [1. 1. 1. 1.]]\n"
     ]
    }
   ],
   "source": [
    "# placeholders\n",
    "t = tf.placeholder(tf.float32)\n",
    "s = tf.placeholder(tf.float32)\n",
    "logits = tf.placeholder(tf.float32)\n",
    "y_true = tf.placeholder(tf.float32)\n",
    "epsilon = 1.e-9\n",
    "\n",
    "zeros = array_ops.zeros_like(logits, dtype=logits.dtype)\n",
    "ones = array_ops.ones_like(logits, dtype=logits.dtype)\n",
    "\n",
    "logit_y = tf.reduce_sum(tf.multiply(y_true, logits), axis=-1, keepdims=True)\n",
    "I_k = array_ops.where(logit_y >= logits, zeros, ones)\n",
    "\n",
    "h = tf.exp(s * tf.multiply(t - 1., tf.multiply(logits + 1., I_k)))\n",
    "\n",
    "\n",
    "# Let's check\n",
    "logits_array = np.array([[2., 3., 1., -1.], [-1., 2.1, 2., 6], [-2., 3., 4, -2.1]])\n",
    "y_true_array = np.array([[0., 1., 0., 0], [0., 0., 1., 0], [1., 0., 0., 0.]])\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    h_array_12 = (sess.run(h, feed_dict={t: 1.2, s: 1, logits: logits_array, y_true: y_true_array}))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    h_array_1 = (sess.run(h, feed_dict={t: 1., s: 1, logits: logits_array, y_true: y_true_array}))\n",
    "    \n",
    "print(\"h with t=1.2:\")\n",
    "print(h_array_12)\n",
    "print('')\n",
    "print(\"h with t=1.0:\")\n",
    "print(h_array_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Full loss is formulated:\n",
    "$$\\mathcal{L} = -log\\frac{e^{s\\cos(\\theta_{w_y}, x)}}{e^{s\\cos(\\theta_{w_y}, x)}+\\sum_{k\\ne y}^Kh(t, θ_{w_k}, x, I_k)e^{s\\cos(\\theta_{w_k, x})}}$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Softmax with t=1.2:\n",
      "[[2.4178252e-01 6.5723300e-01 8.8946812e-02 1.2037643e-02]\n",
      " [2.2175812e-04 4.9225753e-03 4.4541308e-03 2.4318731e-01]\n",
      " [6.9986947e-04 1.0386984e-01 2.8234750e-01 6.3326815e-04]]\n",
      "\n",
      "Softmax with t=1.0:\n",
      "[[2.4178252e-01 6.5723300e-01 8.8946812e-02 1.2037643e-02]\n",
      " [8.7725709e-04 1.9473307e-02 1.7620180e-02 9.6202922e-01]\n",
      " [1.8058796e-03 2.6801631e-01 7.2854382e-01 1.6340276e-03]]\n",
      "\n",
      "Pure softmax:\n",
      "[[2.4178253e-01 6.5723306e-01 8.8946819e-02 1.2037644e-02]\n",
      " [8.7725703e-04 1.9473307e-02 1.7620180e-02 9.6202922e-01]\n",
      " [1.8058793e-03 2.6801628e-01 7.2854376e-01 1.6340273e-03]]\n",
      "\n",
      "Maximum absolute error between our and tf sodtmax:\n",
      "5.9604645e-08\n"
     ]
    }
   ],
   "source": [
    "# placeholders\n",
    "t = tf.placeholder(tf.float32)\n",
    "s = tf.placeholder(tf.float32)\n",
    "logits = tf.placeholder(tf.float32)\n",
    "y_true = tf.placeholder(tf.float32)\n",
    "epsilon = 1.e-9\n",
    "\n",
    "zeros = array_ops.zeros_like(logits, dtype=logits.dtype)\n",
    "ones = array_ops.ones_like(logits, dtype=logits.dtype)\n",
    "\n",
    "logit_y = tf.reduce_sum(tf.multiply(y_true, logits), axis=-1, keepdims=True)\n",
    "I_k = array_ops.where(logit_y >= logits, zeros, ones)\n",
    "\n",
    "h = tf.exp(s * tf.multiply(t - 1., tf.multiply(logits + 1., I_k)))\n",
    "\n",
    "\n",
    "softmax = tf.exp(s * logits) / (tf.reshape(\n",
    "                 tf.reduce_sum(tf.multiply(tf.exp(s * logits), h), axis=-1, keepdims=True), \n",
    "                 [-1, 1]) + epsilon)\n",
    "\n",
    "tf_softmax = tf.nn.softmax(logits)\n",
    "\n",
    "# Let's check softmax\n",
    "logits_array = np.array([[2., 3., 1., -1.], [-1., 2.1, 2., 6], [-2., 3., 4, -2.1]])\n",
    "y_true_array = np.array([[0., 1., 0., 0], [0., 0., 1., 0], [1., 0., 0., 0.]])\n",
    "    \n",
    "with tf.Session() as sess:\n",
    "    softmax_array_12 = (sess.run(softmax, feed_dict={t: 1.2, s: 1, logits: logits_array, y_true: y_true_array}))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    softmax_array_1 = (sess.run(softmax, feed_dict={t: 1., s: 1, logits: logits_array, y_true: y_true_array}))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    tf_softmax_array = (sess.run(tf_softmax, feed_dict={t: 1., s: 1, logits: logits_array, y_true: y_true_array}))\n",
    "    \n",
    "print(\"Softmax with t=1.2:\")\n",
    "print(softmax_array_12)\n",
    "print('')\n",
    "print(\"Softmax with t=1.0:\")\n",
    "print(softmax_array_1)\n",
    "print('')\n",
    "print(\"Pure softmax:\")\n",
    "print(tf_softmax_array)\n",
    "print('')\n",
    "print(\"Maximum absolute error between our and tf sodtmax:\")\n",
    "print(abs((tf_softmax_array-softmax_array_1)).max())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5917118\n"
     ]
    }
   ],
   "source": [
    "# placeholders\n",
    "t = tf.placeholder(tf.float32)\n",
    "m = tf.placeholder(tf.float32)\n",
    "logits = tf.placeholder(tf.float32)\n",
    "y_true = tf.placeholder(tf.float32)\n",
    "\n",
    "\n",
    "zeros = array_ops.zeros_like(logits, dtype=logits.dtype)\n",
    "ones = array_ops.ones_like(logits, dtype=logits.dtype)\n",
    "\n",
    "logit_y = tf.reduce_sum(tf.multiply(y_true, logits), axis=-1, keepdims=True)\n",
    "I_k = array_ops.where(logit_y >= logits, zeros, ones)\n",
    "I_k = array_ops.where(logit_y - m >= logits, zeros, ones)\n",
    "I_k_ = I_k * tf.cast(tf.not_equal(y_true, 1), tf.float32)\n",
    "h = tf.exp(tf.multiply(t - 1., tf.multiply(logits + 1., I_k)))\n",
    "\n",
    "softmax = tf.exp(logits - m * y_true) / tf.reduce_sum(tf.multiply(tf.exp(logits - m * y_true), h), \n",
    "                                                      axis=-1, keepdims=True)\n",
    "# softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis=-1, keepdims=True)\n",
    "\n",
    "# softmax = tf.nn.softmax(logits)\n",
    "\n",
    "# ce = tf.multiply(y_true, -tf.log(softmax))\n",
    "# ce = tf.reduce_sum(ce, axis=1)\n",
    "# ce = tf.reduce_mean(ce)\n",
    "# ce = tf.reduce_mean(-tf.reduce_sum(y_true * tf.log(softmax), reduction_indices=[1]))\n",
    "# ce = tf.losses.softmax_cross_entropy(y_true, softmax)\n",
    "# ce = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_true, logits=logits) \n",
    "# ce = tf.reduce_mean(ce)\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    logits_array = np.array([[2., 3., 1., -1.], [-1., 2.1, 2., 6], [-2., 3., 4, -2.1]])\n",
    "    y_true_array = np.array([[0., 1., 0., 0], [0., 0., 1., 0], [1., 0., 0., 0.]])\n",
    "    print(sess.run(ce, feed_dict={s: 1, t: 1.2, m: 0., logits: logits_array, y_true: y_true_array}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.3660855\n"
     ]
    }
   ],
   "source": [
    "# placeholders\n",
    "epsilon = 1.e-9\n",
    "s = tf.placeholder(tf.float32)\n",
    "t = tf.placeholder(tf.float32)\n",
    "m = tf.placeholder(tf.float32)\n",
    "logits = tf.placeholder(tf.float32)\n",
    "y_true = tf.placeholder(tf.float32)\n",
    "\n",
    "zeros = array_ops.zeros_like(logits, dtype=logits.dtype)\n",
    "ones = array_ops.ones_like(logits, dtype=logits.dtype)\n",
    "\n",
    "# score of groundtruth\n",
    "logit_y = tf.reduce_sum(tf.multiply(y_true, logits), axis=-1, keepdims=True)\n",
    "\n",
    "# binary mask for support vectors\n",
    "I_k = array_ops.where(logit_y >= logits, zeros, ones)\n",
    "\n",
    "# indicator function\n",
    "h = tf.exp(s * tf.multiply(t - 1., tf.multiply(logits + 1., I_k)))\n",
    "\n",
    "softmax = tf.exp(s * logits) / (tf.reshape(\n",
    "                 tf.reduce_sum(tf.multiply(tf.exp(s * logits), h), axis=-1, keepdims=True), \n",
    "                 [-1, 1]) + epsilon)\n",
    "ce = tf.reduce_mean(-tf.reduce_sum(y_true * tf.log(softmax), reduction_indices=[1]))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    logits_array = np.array([[2., 3., 1., -1.], [-1., 2.1, 2., 6], [-2., 3., 4, -2.1]])\n",
    "    y_true_array = np.array([[0., 1., 0., 0], [0., 0., 1., 0], [1., 0., 0., 0.]])\n",
    "    print(sess.run(ce, feed_dict={s: 1, t: 1.2, m: 0., logits: logits_array, y_true: y_true_array}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.3660855\n"
     ]
    }
   ],
   "source": [
    "# placeholders\n",
    "epsilon = 1.e-9\n",
    "s = tf.placeholder(tf.float32)\n",
    "t = tf.placeholder(tf.float32)\n",
    "m = tf.placeholder(tf.float32)\n",
    "logits = tf.placeholder(tf.float32)\n",
    "y_true = tf.placeholder(tf.float32)\n",
    "\n",
    "zeros = array_ops.zeros_like(logits, dtype=logits.dtype)\n",
    "ones = array_ops.ones_like(logits, dtype=logits.dtype)\n",
    "\n",
    "# score of groundtruth\n",
    "logit_y = tf.reduce_sum(tf.multiply(y_true, logits), axis=-1, keepdims=True)\n",
    "\n",
    "# binary mask for support vectors\n",
    "I_k = array_ops.where(logit_y >= logits, zeros, ones)\n",
    "\n",
    "# indicator function\n",
    "h = tf.exp(s * tf.multiply(t - 1., tf.multiply(logits + 1., I_k)))\n",
    "\n",
    "h = s * tf.multiply(t - 1., tf.multiply(logits + 1., I_k))\n",
    "ce = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_true, logits=tf.add(s * logits, h))\n",
    "ce = tf.reduce_mean(ce)\n",
    "# softmax = tf.exp(s * logits) / (tf.reshape(\n",
    "#                  tf.reduce_sum(tf.multiply(tf.exp(s * logits), h), axis=-1, keepdims=True), \n",
    "#                  [-1, 1]) + epsilon)\n",
    "# ce = tf.reduce_mean(-tf.reduce_mean(y_true * tf.log(softmax), reduction_indices=[1]))\n",
    "\n",
    "with tf.Session() as sess:\n",
    "    logits_array = np.array([[2., 3., 1., -1.], [-1., 2.1, 2., 6], [-2., 3., 4, -2.1]])\n",
    "    y_true_array = np.array([[0., 1., 0., 0], [0., 0., 1., 0], [1., 0., 0., 0.]])\n",
    "    print(sess.run(ce, feed_dict={s: 1, t: 1.2, m: 0., logits: logits_array, y_true: y_true_array}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5917118\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    logits_array = np.array([[2., 3., 1., -1.], [-1., 2.1, 2., 6], [-2., 3., 4, -2.1]])\n",
    "    y_true_array = np.array([[0., 1., 0., 0], [0., 0., 1., 0], [1., 0., 0., 0.]])\n",
    "    print(sess.run(ce, feed_dict={s: 1, t: 1.05, m: 0., logits: logits_array, y_true: y_true_array}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[2.4178253e-01 6.5723306e-01 8.8946819e-02 1.2037644e-02]\n",
      " [8.7725703e-04 1.9473307e-02 1.7620180e-02 9.6202922e-01]\n",
      " [1.8058793e-03 2.6801628e-01 7.2854376e-01 1.6340273e-03]]\n"
     ]
    }
   ],
   "source": [
    "with tf.Session() as sess:\n",
    "    logits_array = np.array([[2., 3., 1., -1.], [-1., 2.1, 2., 6], [-2., 3., 4, -2.1]])\n",
    "    y_true_array = np.array([[0., 1., 0., 0], [0., 0., 1., 0], [1., 0., 0., 0.]])\n",
    "    print(sess.run(softmax, feed_dict={s: 1, t: 1.05, m: 0., logits: logits_array, y_true: y_true_array}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n",
    "\n",
    "def sv_softmax_loss(t=1.0, s=1):\n",
    "\n",
    "    t = float(t)\n",
    "    s = float(s)\n",
    "    \n",
    "    def sv_softmax_loss_fixed(y_true, logits):\n",
    "        \"\"\"SV-Softmax loss\n",
    "        Notice: y_pred is raw logits\n",
    "        Support Vector Guided Softmax Loss for Face Recognition\n",
    "        https://arxiv.org/pdf/1812.11317.pdf\n",
    "\n",
    "        Arguments:\n",
    "            y_true {tensor} -- ground truth labels, shape of [batch_size, num_cls]\n",
    "            y_pred {tensor} -- model's output, shape of [batch_size, num_cls]\n",
    "\n",
    "        Keyword Arguments:\n",
    "\n",
    "        Returns:\n",
    "            [tensor] -- loss.\n",
    "        \"\"\"\n",
    "        epsilon = 1.e-9\n",
    "        zeros = array_ops.zeros_like(logits, dtype=logits.dtype)\n",
    "        ones = array_ops.ones_like(logits, dtype=logits.dtype)\n",
    "        \n",
    "        logit_y = tf.reduce_sum(tf.multiply(y_true, logits), axis=-1, keepdims=True)\n",
    "        I_k = array_ops.where(logit_y >= logits, zeros, ones)\n",
    "        \n",
    "        h = tf.exp(s * tf.multiply(t - 1., tf.multiply(logits + 1., I_k)))\n",
    "        \n",
    "        softmax = tf.exp(s * logits) / (tf.reshape(\n",
    "                         tf.reduce_sum(tf.multiply(tf.exp(s * logits), h), axis=-1, keepdims=True), \n",
    "                         [-1, 1]) + epsilon)\n",
    "        \n",
    "        # We add epsilon because log(0) = nan\n",
    "        softmax = tf.add(softmax, epsilon)\n",
    "        ce = tf.multiply(y_true, -tf.log(softmax))\n",
    "        ce = tf.reduce_sum(ce, axis=1)\n",
    "        return tf.reduce_mean(ce)\n",
    "    \n",
    "    return sv_softmax_loss_fixed\n",
    "\n",
    "\n",
    "def sv_am_softmax_loss(t=1.0, s=1, m=0.35):\n",
    "\n",
    "    t = float(t)\n",
    "    s = float(s)\n",
    "    m = float(m)\n",
    "    \n",
    "    def sv_am_softmax_loss_fixed(y_true, logits):\n",
    "        \"\"\"Softmax loss for multi-classification\n",
    "        Notice: y_pred is raw logits\n",
    "        Support Vector Guided Softmax Loss for Face Recognition\n",
    "        https://arxiv.org/pdf/1812.11317.pdf\n",
    "\n",
    "        Arguments:\n",
    "            y_true {tensor} -- ground truth labels, shape of [batch_size, num_cls]\n",
    "            y_pred {tensor} -- model's output, shape of [batch_size, num_cls]\n",
    "\n",
    "        Keyword Arguments:\n",
    "\n",
    "        Returns:\n",
    "            [tensor] -- loss.\n",
    "        \"\"\"\n",
    "        epsilon = 1.e-9\n",
    "        zeros = array_ops.zeros_like(logits, dtype=logits.dtype)\n",
    "        ones = array_ops.ones_like(logits, dtype=logits.dtype)\n",
    "        \n",
    "        logit_y = tf.reduce_sum(tf.multiply(y_true, logits), axis=-1, keepdims=True)\n",
    "        I_k = array_ops.where(logit_y - m >= logits, zeros, ones)\n",
    "        \n",
    "        # I_k should be zero for GT score\n",
    "        I_k = I_k * tf.cast(tf.not_equal(y_true, 1), tf.float32)\n",
    "        # h = tf.exp(s * tf.multiply(t - 1., tf.multiply(logits + 1., I_k)))\n",
    "\n",
    "#         logits = logits - m * y_true\n",
    "#         softmax = tf.exp(logits- m * y_true) / tf.reduce_sum(tf.multiply(tf.exp(logits- m * y_true), h), \n",
    "#                                                  axis=-1, keepdims=True)\n",
    "\n",
    "#         softmax = tf.exp(s * (logits- m * y_true)) / (tf.reshape(\n",
    "#                   tf.reduce_sum(tf.multiply(tf.exp(s * (logits- m * y_true)), h), axis=-1, keepdims=True), \n",
    "#                   [-1, 1]) + epsilon)\n",
    "        \n",
    "        # We add epsilon because log(0) = nan\n",
    "#         softmax = tf.add(softmax, epsilon)\n",
    "#         ce = tf.multiply(y_true, -tf.log(softmax))\n",
    "#         ce = tf.reduce_sum(ce, axis=1)\n",
    "\n",
    "        h = s * tf.multiply(t - 1., tf.multiply(logits + 1., I_k))\n",
    "        ce = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_true, logits=tf.add(s * (logits- m * y_true), h))\n",
    "        # ce = tf.reduce_mean(ce)\n",
    "        return tf.reduce_mean(ce)\n",
    "    \n",
    "    return sv_am_softmax_loss_fixed\n",
    "\n",
    "\n",
    "def focal_loss(gamma=2., alpha=4.):\n",
    "\n",
    "    gamma = float(gamma)\n",
    "    alpha = float(alpha)\n",
    "\n",
    "    def focal_loss_fixed(y_true, y_pred):\n",
    "        \"\"\"Focal loss for multi-classification\n",
    "        FL(p_t)=-alpha(1-p_t)^{gamma}ln(p_t)\n",
    "        Notice: y_pred is probability after softmax\n",
    "        gradient is d(Fl)/d(p_t) not d(Fl)/d(x) as described in paper\n",
    "        d(Fl)/d(p_t) * [p_t(1-p_t)] = d(Fl)/d(x)\n",
    "        Focal Loss for Dense Object Detection\n",
    "        https://arxiv.org/abs/1708.02002\n",
    "\n",
    "        Arguments:\n",
    "            y_true {tensor} -- ground truth labels, shape of [batch_size, num_cls]\n",
    "            y_pred {tensor} -- model's output, shape of [batch_size, num_cls]\n",
    "\n",
    "        Keyword Arguments:\n",
    "            gamma {float} -- (default: {2.0})\n",
    "            alpha {float} -- (default: {4.0})\n",
    "\n",
    "        Returns:\n",
    "            [tensor] -- loss.\n",
    "        \"\"\"\n",
    "        epsilon = 1.e-9\n",
    "        y_true = tf.convert_to_tensor(y_true, tf.float32)\n",
    "        y_pred = tf.convert_to_tensor(y_pred, tf.float32)\n",
    "        y_pred = tf.nn.softmax(y_pred)\n",
    "        \n",
    "        model_out = tf.add(y_pred, epsilon)\n",
    "        ce = tf.multiply(y_true, -tf.log(model_out))\n",
    "        weight = tf.multiply(y_true, tf.pow(tf.subtract(1., model_out), gamma))\n",
    "        fl = tf.multiply(alpha, tf.multiply(weight, ce))\n",
    "        reduced_fl = tf.reduce_max(fl, axis=1)\n",
    "        return tf.reduce_mean(reduced_fl)\n",
    "    return focal_loss_fixed\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
