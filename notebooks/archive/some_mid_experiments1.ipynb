{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "# -*- coding: utf-8 -*-\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras.layers import Dense, Activation, BatchNormalization\n",
    "from keras.layers import activations, initializers, regularizers, constraints, Lambda\n",
    "from keras.engine import InputSpec\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class AMSoftmax(Layer):\n",
    "    def __init__(self, units, s, m,\n",
    "                 kernel_initializer='glorot_uniform',\n",
    "                 kernel_regularizer=None,\n",
    "                 kernel_constraint=None,\n",
    "                 **kwargs\n",
    "                 ):\n",
    "        if 'input_shape' not in kwargs and 'input_dim' in kwargs:\n",
    "            kwargs['input_shape'] = (kwargs.pop('input_dim'),)\n",
    "        super(AMSoftmax, self).__init__(**kwargs)\n",
    "        self.units = units\n",
    "        self.s = s\n",
    "        self.m = m\n",
    "        self.kernel_initializer = initializers.get(kernel_initializer)\n",
    "        self.kernel_regularizer = regularizers.get(kernel_regularizer)\n",
    "        self.kernel_constraint = constraints.get(kernel_constraint)\n",
    "        self.input_spec = InputSpec(min_ndim=2)\n",
    "        self.supports_masking = True\n",
    "\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) >= 2\n",
    "        input_dim = input_shape[-1]\n",
    "\n",
    "        self.kernel = self.add_weight(shape=(input_dim, self.units),\n",
    "                                      initializer=self.kernel_initializer,\n",
    "                                      name='kernel',\n",
    "                                      regularizer=self.kernel_regularizer,\n",
    "                                      constraint=self.kernel_constraint)\n",
    "        self.bias = None\n",
    "\n",
    "        self.input_spec = InputSpec(min_ndim=2, axes={-1: input_dim})\n",
    "        self.built = True\n",
    "\n",
    "\n",
    "    def call(self, inputs, **kwargs):\n",
    "        inputs = tf.nn.l2_normalize(inputs, dim=-1)\n",
    "        self.kernel = tf.nn.l2_normalize(self.kernel, dim=(0, 1))   # W归一化\n",
    "\n",
    "        dis_cosin = K.dot(inputs, self.kernel)\n",
    "        # psi = dis_cosin - self.m\n",
    "\n",
    "        # e_costheta = K.exp(self.s * dis_cosin)\n",
    "        # e_psi = K.exp(self.s * psi)\n",
    "        # sum_x = K.sum(e_costheta, axis=-1, keepdims=True)\n",
    "\n",
    "        # temp = e_psi - e_costheta\n",
    "        # temp = temp + sum_x\n",
    "\n",
    "        # output = e_psi / temp\n",
    "        return dis_cosin\n",
    "\n",
    "\n",
    "def amsoftmax_loss(y_true, y_pred):\n",
    "    d1 = K.sum(y_true * y_pred, axis=-1)\n",
    "    d1 = K.log(K.clip(d1, K.epsilon(), None))\n",
    "    loss = -K.mean(d1, axis=-1)\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From <ipython-input-4-545bf718ca62>:21: read_data_sets (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:260: maybe_download (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please write your own downloading logic.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/base.py:252: _internal_retry.<locals>.wrap.<locals>.wrapped_fn (from tensorflow.contrib.learn.python.learn.datasets.base) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use urllib or similar directly.\n",
      "Successfully downloaded train-images-idx3-ubyte.gz 9912422 bytes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:262: extract_images (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-images-idx3-ubyte.gz\n",
      "Successfully downloaded train-labels-idx1-ubyte.gz 28881 bytes.\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:267: extract_labels (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.data to implement this functionality.\n",
      "Extracting MNIST_data/train-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:110: dense_to_one_hot (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use tf.one_hot on tensors.\n",
      "Successfully downloaded t10k-images-idx3-ubyte.gz 1648877 bytes.\n",
      "Extracting MNIST_data/t10k-images-idx3-ubyte.gz\n",
      "Successfully downloaded t10k-labels-idx1-ubyte.gz 4542 bytes.\n",
      "Extracting MNIST_data/t10k-labels-idx1-ubyte.gz\n",
      "WARNING:tensorflow:From /usr/local/lib/python3.5/dist-packages/tensorflow/contrib/learn/python/learn/datasets/mnist.py:290: DataSet.__init__ (from tensorflow.contrib.learn.python.learn.datasets.mnist) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use alternatives such as official/mnist/dataset.py from tensorflow/models.\n",
      "(55000, 28, 28, 1) (10000, 28, 28, 1) (55000, 10) (10000, 10)\n",
      "(55000, 784) (10000, 784) (55000, 10) (10000, 10)\n",
      "55000 train samples\n",
      "10000 test samples\n",
      "WARNING:tensorflow:From <ipython-input-3-6a95c5171ee8>:46: calling l2_normalize (from tensorflow.python.ops.nn_impl) with dim is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "dim is deprecated, use axis instead\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "input_1 (InputLayer)         (None, 784)               0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 500)               392500    \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 500)               250500    \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 500)               0         \n",
      "_________________________________________________________________\n",
      "am_softmax_1 (AMSoftmax)     (None, 500)               5000      \n",
      "=================================================================\n",
      "Total params: 648,000\n",
      "Trainable params: 648,000\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n",
      "Train on 55000 samples, validate on 10000 samples\n",
      "Epoch 1/10\n",
      "55000/55000 [==============================] - 2s 44us/step - loss: 3.6102 - acc: 0.8740 - val_loss: 2.8146 - val_acc: 0.9406\n",
      "Epoch 2/10\n",
      "55000/55000 [==============================] - 2s 40us/step - loss: 3.0701 - acc: 0.9407 - val_loss: 2.6992 - val_acc: 0.9493\n",
      "Epoch 3/10\n",
      "55000/55000 [==============================] - 2s 40us/step - loss: 2.9924 - acc: 0.9519 - val_loss: 2.6345 - val_acc: 0.9619\n",
      "Epoch 4/10\n",
      "55000/55000 [==============================] - 2s 40us/step - loss: 2.9773 - acc: 0.9551 - val_loss: 2.6210 - val_acc: 0.9646\n",
      "Epoch 5/10\n",
      "55000/55000 [==============================] - 2s 40us/step - loss: 2.9467 - acc: 0.9620 - val_loss: 2.6159 - val_acc: 0.9669\n",
      "Epoch 6/10\n",
      "55000/55000 [==============================] - 2s 40us/step - loss: 2.9359 - acc: 0.9644 - val_loss: 2.6136 - val_acc: 0.9653\n",
      "Epoch 7/10\n",
      "55000/55000 [==============================] - 2s 40us/step - loss: 2.9233 - acc: 0.9680 - val_loss: 2.5906 - val_acc: 0.9713\n",
      "Epoch 8/10\n",
      "55000/55000 [==============================] - 2s 40us/step - loss: 2.9081 - acc: 0.9713 - val_loss: 2.5981 - val_acc: 0.9698\n",
      "Epoch 9/10\n",
      "55000/55000 [==============================] - 2s 40us/step - loss: 2.9058 - acc: 0.9723 - val_loss: 2.6004 - val_acc: 0.9686\n",
      "Epoch 10/10\n",
      "55000/55000 [==============================] - 2s 40us/step - loss: 2.8998 - acc: 0.9733 - val_loss: 2.5860 - val_acc: 0.9726\n",
      "Test score: 2.58602007713\n",
      "Test accuracy: 0.9726\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(1337)\n",
    "from keras import backend as K\n",
    "from keras.layers import Dense,Input,Conv2D,MaxPooling2D,Dropout,BatchNormalization\n",
    "from keras.models import Model\n",
    "from keras.optimizers import SGD, Adam\n",
    "import os\n",
    "from keras.backend.tensorflow_backend import set_session\n",
    "\n",
    "batch_size = 200\n",
    "nb_classes = 10\n",
    "nb_epoch = 2\n",
    "\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = '1'\n",
    "config = tf.ConfigProto()\n",
    "config.gpu_options.per_process_gpu_memory_fraction = 0.8\n",
    "set_session(tf.Session(config=config))\n",
    "\n",
    "from tensorflow.examples.tutorials.mnist import input_data\n",
    "mnist = input_data.read_data_sets(\"MNIST_data/\", one_hot=True)\n",
    "\n",
    "X_train, Y_train = mnist.train.images,mnist.train.labels\n",
    "X_test, Y_test = mnist.test.images, mnist.test.labels\n",
    "X_train = X_train.reshape(-1, 28, 28,1).astype('float32')\n",
    "X_test = X_test.reshape(-1,28, 28,1).astype('float32')\n",
    "\n",
    "#打印训练数据和测试数据的维度\n",
    "print(X_train.shape,X_test.shape,Y_train.shape,Y_test.shape)\n",
    "\n",
    "#修改维度\n",
    "X_train = X_train.reshape(55000,784)\n",
    "X_test = X_test.reshape(10000,784)\n",
    "print(X_train.shape, X_test.shape, Y_train.shape, Y_test.shape)\n",
    "\n",
    "# 将X_train, X_test的数据格式转为float32存储\n",
    "X_train = X_train.astype('float32')\n",
    "X_test = X_test.astype('float32')\n",
    "# 归一化\n",
    "X_train /= 255\n",
    "X_test /= 255\n",
    "# 打印出训练集和测试集的信息\n",
    "print(X_train.shape[0], 'train samples')\n",
    "print(X_test.shape[0], 'test samples')\n",
    "\n",
    "\n",
    "x_input = Input(shape=(784,))\n",
    "y = Dense(500, activation='relu')(x_input)\n",
    "y = Dropout(0.2)(y)\n",
    "y = Dense(500, activation='relu')(y)\n",
    "y = Dropout(0.2)(y)\n",
    "\n",
    "output = AMSoftmax(10, 10, 0.35)(y)\n",
    "#output = Dense(10, activation='softmax')(y)\n",
    "model = Model(inputs=x_input, outputs=output)\n",
    "model.summary()\n",
    "\n",
    "adam = Adam()\n",
    "model.compile(loss=amsoftmax_loss,\n",
    "              optimizer=adam,\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit(X_train, Y_train,\n",
    "                    batch_size=batch_size,\n",
    "                    epochs=10,\n",
    "                    verbose=1,\n",
    "                    validation_data=(X_test, Y_test))\n",
    "\n",
    "score = model.evaluate(X_test, Y_test, verbose=0)\n",
    "\n",
    "print('Test score:', score[0])\n",
    "print('Test accuracy:', score[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'AMSoftmax' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-34-4401530b27e7>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mActivation\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'relu'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlayers\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mDropout\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m0.5\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 24\u001b[0;31m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0madd\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mAMSoftmax\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m100\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m0.35\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     25\u001b[0m \u001b[0;31m# model.add(layers.Dense(100))\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'AMSoftmax' is not defined"
     ]
    }
   ],
   "source": [
    "from keras import *\n",
    "\n",
    "model = models.Sequential()\n",
    "\n",
    "model.add(layers.Convolution2D(32, 3, padding='same',\n",
    "                               input_shape=(32, 32, 3)))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Convolution2D(32, 3))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(layers.Dropout(0.25))\n",
    "\n",
    "model.add(layers.Convolution2D(64, 3, padding='same'))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Convolution2D(64, 3))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(layers.Dropout(0.25))\n",
    "\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512))\n",
    "model.add(layers.Activation('relu'))\n",
    "model.add(layers.Dropout(0.5))\n",
    "model.add(AMSoftmax(100, 10, 0.35))\n",
    "# model.add(layers.Dense(100))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "(train_images, train_labels), (test_images, test_labels) = datasets.cifar100.load_data()\n",
    "\n",
    "train_images = train_images.reshape((50000, 32, 32, 3))\n",
    "test_images = test_images.reshape((10000, 32, 32, 3))\n",
    "\n",
    "# Normalize pixel values to be between 0 and 1\n",
    "train_images, test_images = train_images / 255.0, test_images / 255.0\n",
    "# train_images, test_images = train_images - 0.5, test_images - 0.5\n",
    "# train_images, test_images = train_images * 2, test_images * 2\n",
    "\n",
    "# To one-hot\n",
    "train_labels = utils.to_categorical(train_labels, 100)\n",
    "test_labels = utils.to_categorical(test_labels, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.ops import array_ops\n",
    "\n",
    "\n",
    "def softmax_loss(t=1.0, s=10):\n",
    "\n",
    "    t = float(t)\n",
    "    s = float(s)\n",
    "    \n",
    "    def softmax_loss_fixed(y_true, logits):\n",
    "        \"\"\"Softmax loss for multi-classification\n",
    "        FL(p_t)=-alpha(1-p_t)^{gamma}ln(p_t)\n",
    "        Notice: y_pred is raw logits\n",
    "        Focal Loss for Dense Object Detection\n",
    "        https://arxiv.org/abs/1708.02002\n",
    "\n",
    "        Arguments:\n",
    "            y_true {tensor} -- ground truth labels, shape of [batch_size, num_cls]\n",
    "            y_pred {tensor} -- model's output, shape of [batch_size, num_cls]\n",
    "\n",
    "        Keyword Arguments:\n",
    "\n",
    "        Returns:\n",
    "            [tensor] -- loss.\n",
    "        \"\"\"\n",
    "        epsilon = 1.e-9\n",
    "        zeros = array_ops.zeros_like(logits, dtype=logits.dtype)\n",
    "        ones = array_ops.ones_like(logits, dtype=logits.dtype)\n",
    "        \n",
    "        # Возможно косяк здесь! Хз как правильно искать значение логита на тру-классе\n",
    "        logit_y = tf.reduce_sum(tf.multiply(y_true, logits), axis=-1, keepdims=True)\n",
    "        I_k = array_ops.where(logit_y >= logits, zeros, ones)\n",
    "        \n",
    "        h = tf.exp(s*tf.multiply(t - 1., tf.multiply(logits + ones, I_k)))\n",
    "        \n",
    "        # softmax = tf.exp(logits) / tf.reduce_sum(tf.multiply(tf.exp(logits), h))\n",
    "#         softmax = tf.exp(logits) / (tf.reshape(\n",
    "#             tf.reduce_sum(tf.multiply(tf.exp(logits), h)), \n",
    "#             [-1, 1]) + epsilon)\n",
    "        softmax = tf.exp(s*logits) / (tf.reduce_sum(tf.multiply(tf.exp(s*logits), h), axis=-1, keepdims=True) + epsilon)\n",
    "        # softmax = tf.nn.softmax(logits)\n",
    "        # softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits))\n",
    "        \n",
    "        # We add epsilon because log(0) = nan\n",
    "        softmax = tf.add(softmax, epsilon)\n",
    "        ce = tf.multiply(y_true, -tf.log(softmax))\n",
    "        return tf.reduce_mean(ce)\n",
    "    \n",
    "    return softmax_loss_fixed"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 50000 samples, validate on 10000 samples\n",
      "Epoch 1/50\n",
      "50000/50000 [==============================] - 8s 159us/step - loss: 0.0971 - acc: 0.0766 - val_loss: 0.0914 - val_acc: 0.1561\n",
      "Epoch 2/50\n",
      "50000/50000 [==============================] - 8s 156us/step - loss: 0.0909 - acc: 0.1711 - val_loss: 0.0879 - val_acc: 0.2223\n",
      "Epoch 3/50\n",
      "50000/50000 [==============================] - 8s 156us/step - loss: 0.0887 - acc: 0.2145 - val_loss: 0.0862 - val_acc: 0.2598\n",
      "Epoch 4/50\n",
      "50000/50000 [==============================] - 8s 156us/step - loss: 0.0873 - acc: 0.2480 - val_loss: 0.0848 - val_acc: 0.2884\n",
      "Epoch 5/50\n",
      "50000/50000 [==============================] - 8s 156us/step - loss: 0.0861 - acc: 0.2740 - val_loss: 0.0836 - val_acc: 0.3150\n",
      "Epoch 6/50\n",
      "50000/50000 [==============================] - 8s 156us/step - loss: 0.0852 - acc: 0.2966 - val_loss: 0.0827 - val_acc: 0.3329\n",
      "Epoch 7/50\n",
      "50000/50000 [==============================] - 8s 156us/step - loss: 0.0844 - acc: 0.3177 - val_loss: 0.0822 - val_acc: 0.3483\n",
      "Epoch 8/50\n",
      "50000/50000 [==============================] - 8s 156us/step - loss: 0.0837 - acc: 0.3334 - val_loss: 0.0819 - val_acc: 0.3529\n",
      "Epoch 9/50\n",
      "50000/50000 [==============================] - 8s 156us/step - loss: 0.0832 - acc: 0.3494 - val_loss: 0.0815 - val_acc: 0.3632\n",
      "Epoch 10/50\n",
      "50000/50000 [==============================] - 8s 151us/step - loss: 0.0827 - acc: 0.3602 - val_loss: 0.0809 - val_acc: 0.3787\n",
      "Epoch 11/50\n",
      "50000/50000 [==============================] - 8s 153us/step - loss: 0.0823 - acc: 0.3721 - val_loss: 0.0806 - val_acc: 0.3844\n",
      "Epoch 12/50\n",
      "50000/50000 [==============================] - 8s 158us/step - loss: 0.0820 - acc: 0.3806 - val_loss: 0.0805 - val_acc: 0.3824\n",
      "Epoch 13/50\n",
      "50000/50000 [==============================] - 8s 158us/step - loss: 0.0818 - acc: 0.3885 - val_loss: 0.0800 - val_acc: 0.3985\n",
      "Epoch 14/50\n",
      "50000/50000 [==============================] - 8s 153us/step - loss: 0.0814 - acc: 0.3987 - val_loss: 0.0800 - val_acc: 0.3992\n",
      "Epoch 15/50\n",
      "50000/50000 [==============================] - 8s 157us/step - loss: 0.0812 - acc: 0.4059 - val_loss: 0.0797 - val_acc: 0.4112\n",
      "Epoch 16/50\n",
      "50000/50000 [==============================] - 8s 157us/step - loss: 0.0811 - acc: 0.4131 - val_loss: 0.0798 - val_acc: 0.4096\n",
      "Epoch 17/50\n",
      "50000/50000 [==============================] - 8s 154us/step - loss: 0.0809 - acc: 0.4173 - val_loss: 0.0796 - val_acc: 0.4134\n",
      "Epoch 18/50\n",
      "50000/50000 [==============================] - 8s 155us/step - loss: 0.0808 - acc: 0.4223 - val_loss: 0.0796 - val_acc: 0.4208\n",
      "Epoch 19/50\n",
      "50000/50000 [==============================] - 8s 161us/step - loss: 0.0808 - acc: 0.4285 - val_loss: 0.0802 - val_acc: 0.4136\n",
      "Epoch 20/50\n",
      "50000/50000 [==============================] - 8s 161us/step - loss: 0.0806 - acc: 0.4354 - val_loss: 0.0798 - val_acc: 0.4176\n",
      "Epoch 21/50\n",
      "50000/50000 [==============================] - 8s 161us/step - loss: 0.0805 - acc: 0.4371 - val_loss: 0.0795 - val_acc: 0.4275\n",
      "Epoch 22/50\n",
      "50000/50000 [==============================] - 8s 159us/step - loss: 0.0805 - acc: 0.4426 - val_loss: 0.0797 - val_acc: 0.4288\n",
      "Epoch 23/50\n",
      "50000/50000 [==============================] - 7s 150us/step - loss: 0.0804 - acc: 0.4467 - val_loss: 0.0799 - val_acc: 0.4240\n",
      "Epoch 24/50\n",
      "50000/50000 [==============================] - 8s 157us/step - loss: 0.0802 - acc: 0.4517 - val_loss: 0.0797 - val_acc: 0.4246\n",
      "Epoch 25/50\n",
      "50000/50000 [==============================] - 8s 156us/step - loss: 0.0803 - acc: 0.4525 - val_loss: 0.0802 - val_acc: 0.4194\n",
      "Epoch 26/50\n",
      "50000/50000 [==============================] - 8s 155us/step - loss: 0.0802 - acc: 0.4552 - val_loss: 0.0799 - val_acc: 0.4299\n",
      "Epoch 27/50\n",
      "50000/50000 [==============================] - 8s 155us/step - loss: 0.0800 - acc: 0.4614 - val_loss: 0.0797 - val_acc: 0.4340\n",
      "Epoch 28/50\n",
      "50000/50000 [==============================] - 8s 155us/step - loss: 0.0799 - acc: 0.4667 - val_loss: 0.0800 - val_acc: 0.4289\n",
      "Epoch 29/50\n",
      "50000/50000 [==============================] - 8s 155us/step - loss: 0.0799 - acc: 0.4705 - val_loss: 0.0814 - val_acc: 0.4034\n",
      "Epoch 30/50\n",
      "50000/50000 [==============================] - 7s 149us/step - loss: 0.0799 - acc: 0.4699 - val_loss: 0.0796 - val_acc: 0.4351\n",
      "Epoch 31/50\n",
      "50000/50000 [==============================] - 8s 157us/step - loss: 0.0799 - acc: 0.4679 - val_loss: 0.0798 - val_acc: 0.4341\n",
      "Epoch 32/50\n",
      "50000/50000 [==============================] - 8s 150us/step - loss: 0.0799 - acc: 0.4737 - val_loss: 0.0797 - val_acc: 0.4391\n",
      "Epoch 33/50\n",
      "50000/50000 [==============================] - 8s 155us/step - loss: 0.0797 - acc: 0.4760 - val_loss: 0.0797 - val_acc: 0.4291\n",
      "Epoch 34/50\n",
      "50000/50000 [==============================] - 8s 155us/step - loss: 0.0797 - acc: 0.4779 - val_loss: 0.0799 - val_acc: 0.4341\n",
      "Epoch 35/50\n",
      "50000/50000 [==============================] - 8s 153us/step - loss: 0.0797 - acc: 0.4789 - val_loss: 0.0797 - val_acc: 0.4388\n",
      "Epoch 36/50\n",
      "50000/50000 [==============================] - 8s 153us/step - loss: 0.0795 - acc: 0.4846 - val_loss: 0.0798 - val_acc: 0.4426\n",
      "Epoch 37/50\n",
      "50000/50000 [==============================] - 8s 153us/step - loss: 0.0795 - acc: 0.4882 - val_loss: 0.0801 - val_acc: 0.4349\n",
      "Epoch 38/50\n",
      "50000/50000 [==============================] - 8s 153us/step - loss: 0.0795 - acc: 0.4872 - val_loss: 0.0797 - val_acc: 0.4393\n",
      "Epoch 39/50\n",
      "50000/50000 [==============================] - 8s 153us/step - loss: 0.0795 - acc: 0.4876 - val_loss: 0.0800 - val_acc: 0.4366\n",
      "Epoch 40/50\n",
      "50000/50000 [==============================] - 8s 153us/step - loss: 0.0793 - acc: 0.4968 - val_loss: 0.0801 - val_acc: 0.4318\n",
      "Epoch 41/50\n",
      "50000/50000 [==============================] - 8s 154us/step - loss: 0.0795 - acc: 0.4914 - val_loss: 0.0802 - val_acc: 0.4299\n",
      "Epoch 42/50\n",
      "50000/50000 [==============================] - 8s 159us/step - loss: 0.0814 - acc: 0.4507 - val_loss: 0.0807 - val_acc: 0.4199\n",
      "Epoch 43/50\n",
      "50000/50000 [==============================] - 8s 157us/step - loss: 0.0795 - acc: 0.4918 - val_loss: 0.0804 - val_acc: 0.4314\n",
      "Epoch 44/50\n",
      "50000/50000 [==============================] - 8s 157us/step - loss: 0.0793 - acc: 0.4976 - val_loss: 0.0800 - val_acc: 0.4373\n",
      "Epoch 45/50\n",
      "50000/50000 [==============================] - 8s 156us/step - loss: 0.0794 - acc: 0.4976 - val_loss: 0.0801 - val_acc: 0.4410\n",
      "Epoch 46/50\n",
      "50000/50000 [==============================] - 8s 155us/step - loss: 0.0791 - acc: 0.5012 - val_loss: 0.0799 - val_acc: 0.4403\n",
      "Epoch 47/50\n",
      "50000/50000 [==============================] - 8s 153us/step - loss: 0.0792 - acc: 0.5010 - val_loss: 0.0802 - val_acc: 0.4304\n",
      "Epoch 48/50\n",
      "50000/50000 [==============================] - 8s 157us/step - loss: 0.0790 - acc: 0.5016 - val_loss: 0.0801 - val_acc: 0.4325\n",
      "Epoch 49/50\n",
      "50000/50000 [==============================] - 8s 157us/step - loss: 0.0792 - acc: 0.4991 - val_loss: 0.0797 - val_acc: 0.4421\n",
      "Epoch 50/50\n",
      "50000/50000 [==============================] - 8s 157us/step - loss: 0.0790 - acc: 0.5084 - val_loss: 0.0797 - val_acc: 0.4396\n"
     ]
    }
   ],
   "source": [
    "model.compile(optimizer='adam',\n",
    "              loss=softmax_loss(t=1.1, s=30),\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history1 = model.fit(train_images, train_labels, epochs=50,\n",
    "                     validation_data=(test_images, test_labels));"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "https://github.com/Joker316701882/Additive-Margin-Softmax/issues/9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10000/10000 [==============================] - 1s 69us/step\n",
      "0.4396\n"
     ]
    }
   ],
   "source": [
    "model.add(layers.Activation('softmax'))\n",
    "model.compile(optimizer='adam',\n",
    "              loss=softmax_loss(),\n",
    "              metrics=['accuracy'])\n",
    "test_loss, test_acc = model.evaluate(test_images, test_labels)\n",
    "print(test_acc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.656529\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.python.ops import array_ops\n",
    "\n",
    "t = tf.placeholder(tf.float32)\n",
    "m = tf.placeholder(tf.float32)\n",
    "logits = tf.placeholder(tf.float32)\n",
    "y_true = tf.placeholder(tf.float32)\n",
    "zeros = array_ops.zeros_like(logits, dtype=logits.dtype)\n",
    "ones = array_ops.ones_like(logits, dtype=logits.dtype)\n",
    "\n",
    "logit_y = tf.reduce_sum(tf.multiply(y_true, logits), axis=-1, keepdims=True)\n",
    "I_k = array_ops.where(logit_y >= logits, zeros, ones)\n",
    "I_k = array_ops.where(logit_y - m >= logits, zeros, ones)\n",
    "I_k_ = I_k * tf.cast(tf.not_equal(y_true, 1), tf.float32)\n",
    "h = tf.exp(tf.multiply(t - 1., tf.multiply(logits + 1., I_k)))\n",
    "\n",
    "# logits = logits - m * y_true\n",
    "softmax = tf.exp(logits- m * y_true) / tf.reduce_sum(tf.multiply(tf.exp(logits- m * y_true), h), \n",
    "                                         axis=-1, keepdims=True)\n",
    "# softmax = tf.exp(logits) / tf.reduce_sum(tf.exp(logits), axis=-1, keepdims=True)\n",
    "\n",
    "# softmax = tf.nn.softmax(logits)\n",
    "\n",
    "# ce = tf.multiply(y_true, -tf.log(softmax))\n",
    "# ce = tf.reduce_sum(ce, axis=1)\n",
    "# ce = tf.reduce_mean(ce)\n",
    "ce = tf.reduce_mean(-tf.reduce_sum(y_true * tf.log(softmax), reduction_indices=[1]))\n",
    "# ce = tf.losses.softmax_cross_entropy(y_true, softmax)\n",
    "# ce = tf.nn.softmax_cross_entropy_with_logits_v2(labels=y_true, logits=logits) \n",
    "# ce = tf.reduce_mean(ce)\n",
    "with tf.Session() as sess:\n",
    "  logits_array = np.array([[2., 3., 1.], [1., 2.1, 2.]])\n",
    "  y_true_array = np.array([[0., 1., 0.], [0., 0., 1.]])\n",
    "  print(sess.run(ce, feed_dict={t: 1., m: 0., logits: logits_array, y_true: y_true_array}))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tf.lo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
